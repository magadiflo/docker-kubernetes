# Secci√≥n 15: Kubernetes: Vol√∫menes

---

## [üì¶ Introducci√≥n a los Vol√∫menes en Kubernetes](https://kubernetes.io/docs/concepts/storage/volumes/)

En Kubernetes, `los archivos almacenados dentro de un contenedor son ef√≠meros`, lo que representa ciertos desaf√≠os para
aplicaciones que requieren persistencia de datos o almacenamiento compartido.

### üß® Problemas comunes sin vol√∫menes

1. `P√©rdida de datos al reiniciar el contenedor`: Si un contenedor se bloquea o se detiene, todos los archivos creados o
   modificados durante su ejecuci√≥n se pierden, ya que el sistema de archivos del contenedor se reinicia con un estado
   limpio cuando el `kubelet` lo vuelve a lanzar.


2. `Dificultad para compartir datos entre contenedores del mismo Pod`: En situaciones donde m√∫ltiples contenedores
   comparten un Pod, puede ser complicado habilitar un sistema de archivos com√∫n entre ellos sin una soluci√≥n adecuada.

### ‚úÖ ¬øC√≥mo lo resuelve Kubernetes?

Para abordar estos problemas, `Kubernetes introduce la abstracci√≥n de volumen`. Esta permite proporcionar
almacenamiento persistente o compartido a los contenedores dentro de un Pod.

> üìå Un `volumen en Kubernetes` es b√°sicamente un directorio accesible desde uno o varios contenedores del Pod, que
> puede estar respaldado por distintos medios de almacenamiento, seg√∫n el tipo de volumen utilizado.

### üß± Tipos de vol√∫menes

Kubernetes soporta diversos tipos de vol√∫menes, y un Pod puede montar varios de ellos simult√°neamente. Se clasifican, de
forma general, en:

- `Vol√∫menes ef√≠meros`: Existen √∫nicamente durante la vida del Pod. Cuando este se elimina, el volumen tambi√©n se
  destruye.

- `Vol√∫menes persistentes`: Sobreviven incluso despu√©s de que el Pod se elimina. Su ciclo de vida es independiente del
  Pod, permitiendo que los datos se mantengan y puedan ser reutilizados por nuevos Pods.

> üîÅ Un volumen, sin importar su tipo, conserva sus datos durante los reinicios de los contenedores dentro del
> mismo Pod. Sin embargo, solo los vol√∫menes persistentes sobreviven a la eliminaci√≥n del Pod completo.

### üí° Ejemplo simple

Sup√≥n que tienes un Pod con un volumen `emptyDir` (el volumen se monta dentro del pod), y un contenedor escribe un
archivo ah√≠. Si el contenedor se reinicia por error interno, `el archivo sigue estando presente` en ese volumen cuando
el contenedor se vuelve a iniciar.

Sin embargo, si se elimina el `Pod` completo, entonces:

- Si era un `volumen ef√≠mero` (`emptyDir`), el contenido se borra.
- Si era un `volumen persistente` (como un `persistentVolumeClaim`), los datos se conservan, incluso despu√©s de
  eliminar el Pod.

## [Tipos de vol√∫menes](https://kubernetes.io/docs/concepts/storage/volumes/#volume-types)

Como vimos anteriormente, en Kubernetes existen dos grandes categor√≠as de vol√∫menes:

- `Vol√∫menes ef√≠meros`: viven mientras el Pod exista.
- `Vol√∫menes persistentes`: sobreviven incluso si el Pod es eliminado.

A continuaci√≥n, exploramos algunos de los tipos m√°s comunes.

### [üóÇÔ∏è emptyDir](https://kubernetes.io/docs/concepts/storage/volumes/#emptydir)

Este es uno de los vol√∫menes ef√≠meros m√°s utilizados.

- `El volumen emptyDir se monta dentro del Pod` y su ciclo de vida est√° ligado directamente al del Pod, no al de los
  contenedores individuales.
- Se crea `cuando el Pod es asignado a un nodo`, y como su nombre lo indica, `inicialmente est√° vac√≠o`.
- Todos los contenedores del Pod pueden `leer y escribir archivos compartidos` en este volumen. Pueden montarlo en la
  misma ruta o en rutas distintas seg√∫n se requiera.
- Cuando el `Pod se elimina`, el volumen `emptyDir` tambi√©n se `destruye autom√°ticamente`, junto con todos los datos
  almacenados en √©l. Por eso, se usa com√∫nmente para `almacenar datos temporales`.

üìå Importante:

> La `ca√≠da o reinicio de un contenedor` dentro del Pod `no elimina` el volumen `emptyDir`.
> Los datos almacenados permanecen disponibles mientras el `Pod siga activo en el nodo`.

![01.png](assets/section-15/01.png)

### [üóÇÔ∏è hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath)

El volumen `hostPath` permite montar directamente un archivo o directorio del sistema de archivos del nodo host
(el Worker Node) dentro del Pod.

- Es `externo al Pod`, pero `local al nodo` donde se ejecuta.
- Su uso est√° `limitado a escenarios donde se tiene control total del nodo`, y por lo general
  `solo es viable en cl√∫steres con un solo nodo` o para tareas muy espec√≠ficas.

üìå Se puede usar para casos como:

- Acceder a archivos del host que necesita la aplicaci√≥n (por ejemplo, logs del sistema).
- Exponer sockets de Docker o herramientas de monitoreo.

![02.png](assets/section-15/02.png)

üö® Advertencia de seguridad:

> El uso de `hostPath` conlleva `riesgos importantes de seguridad`, ya que permite al contenedor acceder directamente al
> sistema de archivos del nodo, lo que `puede comprometer el nodo completo` si no se tiene cuidado.
>
> `Si puedes evitar usar hostPath, deber√≠as hacerlo.`
>
> En su lugar, se recomienda utilizar un `PersistentVolume local (local PV)` con las debidas restricciones y
> aislamiento.

üìù Nota:

> En los siguientes apartados usaremos el `hostPath` por ser m√°s simple y directo en entornos de desarrollo o
> pruebas. Sin embargo, m√°s adelante veremos c√≥mo reemplazarlo por una alternativa m√°s segura y desacoplada utilizando
> `PersistentVolume` y `PersistentVolumeClaim`.

### üóÇÔ∏è[nfs](https://kubernetes.io/docs/concepts/storage/volumes/#nfs), [csi](https://kubernetes.io/docs/concepts/storage/volumes/#csi)

Tanto `nfs` como `csi` son tipos de vol√∫menes que permiten el acceso desde `varios Worker Nodes`, por lo que est√°n
dise√±ados para funcionar correctamente en `cl√∫steres multi-nodo`.

Veamos qu√© hace cada uno:

### üìÅ nfs ‚Äì Network File System

- Un volumen `nfs` permite montar un recurso compartido NFS existente en uno o varios Pods.
- A diferencia de `emptyDir`, `el contenido no se elimina cuando el Pod se borra`; simplemente se desmonta.
- Esto permite que el volumen:
    - `Est√© pre-poblado con datos`.
    - `Comparta informaci√≥n entre m√∫ltiples Pods`, incluso si est√°n en nodos distintos.
    - `Admite m√∫ltiples escritores simult√°neamente`, lo cual es ideal para escenarios de acceso concurrente.

> üìå Es importante contar con un servidor NFS configurado y accesible desde todos los nodos del cl√∫ster.

### üì¶ csi ‚Äì Container Storage Interface

- CSI define una `interfaz est√°ndar` y extensible para integrar soluciones de almacenamiento externas con Kubernetes.
- Gracias a CSI, los proveedores de almacenamiento (como AWS EBS, GCP PD, Ceph, NetApp, etc.) pueden ofrecer vol√∫menes
  que Kubernetes puede montar autom√°ticamente.
- Permite que Kubernetes `interact√∫e con cualquier sistema de almacenamiento compatible con CSI`, tanto para cl√∫steres
  on-premise como en la nube.
- CSI reemplaza gradualmente a muchos controladores in-tree antiguos, y es el enfoque recomendado para nuevas
  implementaciones.

![03.png](assets/section-15/03.png)

## üõ†Ô∏è Configurando un volumen hostPath para MySQL

Cuando trabajamos con contenedores en `Kubernetes`, los datos almacenados en el sistema de archivos del contenedor se
eliminan si el contenedor o el Pod se reinicia o destruye. Para evitar esta p√©rdida de datos, debemos montar un volumen
externo al contenedor que persista los datos, incluso si el Pod deja de existir.

### üîç ¬øQu√© tipo de volumen usar?

En este escenario, usaremos el tipo de volumen `hostPath`, que permite montar un directorio local del nodo (el host)
dentro del contenedor. De este modo, aunque el Pod sea eliminado, los datos seguir√°n existiendo en el sistema de
archivos del nodo.

> üí° M√°s adelante aprenderemos a usar una opci√≥n m√°s segura y desacoplada: los `PersistentVolume` y
`PersistentVolumeClaim`.

### üì¶ Definiendo el volumen `hostPath` para MySQL

En este punto queremos configurar el volumen para la base de datos MySQL utilizada por el microservicio de usuarios.
Para ello, modificamos el archivo `deployment-mysql.yml` con la siguiente configuraci√≥n:

````yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: d-mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: d-mysql
  template:
    metadata:
      labels:
        app: d-mysql
    spec:
      containers:
        - image: mysql:8.0.41-debian
          name: c-mysql
          ports:
            - containerPort: 3306
          env:
            - name: MYSQL_ROOT_PASSWORD
              value: magadiflo
            - name: MYSQL_DATABASE
              value: db_user_service
            - name: MYSQL_USER
              value: admin
            - name: MYSQL_PASSWORD
              value: magadiflo
          volumeMounts:
            - name: mysql-data
              mountPath: /var/lib/mysql
      volumes:
        - name: mysql-data
          hostPath:
            path: /var/lib/mysql
            type: DirectoryOrCreate
````

### üß© Estructura de configuraci√≥n

La definici√≥n del volumen consta de dos partes principales:

1. Configuraci√≥n del volumen

````yaml
volumes:
  - name: mysql-data
    hostPath:
      path: /var/lib/mysql
      type: DirectoryOrCreate
````

¬øQu√© significa esto?

- `name: mysql-data`: nombre del volumen, que usaremos como referencia en el contenedor.
- `hostPath.path: /var/lib/mysql`: ruta del sistema de archivos del nodo host donde se almacenar√°n los datos.
  > Esta ruta no tiene que ser igual a la del contenedor, pero es com√∫n usar la misma por coherencia.
- `type: DirectoryOrCreate`: si el directorio ya existe en el nodo, lo reutiliza; si no, lo crea autom√°ticamente.

2. Configuraci√≥n del `volumeMounts` en el contenedor

````yml
volumeMounts:
  - name: mysql-data
    mountPath: /var/lib/mysql
````

¬øQu√© significa esto?

- `name: mysql-data`: nombre del volumen a montar, debe coincidir con el definido en volumes.
- `mountPath: /var/lib/mysql`: ruta dentro del contenedor MySQL donde se almacenar√°n los datos.

> Es muy importante que esta ruta (`mountPath`) coincida exactamente con el directorio que MySQL usa internamente para
> almacenar sus datos. En el caso de la imagen oficial `mysql:8.0.41-debian`, ese directorio es `/var/lib/mysql`.

üìå Nota importante
> Aunque el directorio del host (`hostPath.path`) podr√≠a ser diferente, se recomienda usar la misma ruta que el
> `mountPath` del contenedor para mantener la coherencia y facilitar el mantenimiento.

## Aplicando configuraci√≥n de volumen hostPath para MySQL

Antes de aplicar el `deployment-mysql.yml` donde configuramos el volumen del tipo `hostPath`, vamos a ver qu√© `pods`
tenemos actualmente en ejecuci√≥n.

````bash
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS       AGE
d-course-service-5f7bdcf6dc-ln4pg   1/1     Running   4 (3m9s ago)   5d3h
d-mysql-7b947869d5-fshhr            1/1     Running   5 (3m9s ago)   5d23h
d-postgres-69f7b66859-9j8h7         1/1     Running   5 (3m9s ago)   5d22h
d-user-service-7bc4ffb6df-4zzj8     1/1     Running   17 (53s ago)   5d22h
d-user-service-7bc4ffb6df-fg95f     1/1     Running   17 (53s ago)   5d22h
d-user-service-7bc4ffb6df-pvm4w     1/1     Running   17 (53s ago)   5d22h
d-user-service-7bc4ffb6df-pvm9t     1/1     Running   17 (51s ago)   5d22h
d-user-service-7bc4ffb6df-r54hn     1/1     Running   17 (54s ago)   5d22h
````

Ahora estamos listos para aplicar el deployment `deployment-mysql.yml`.

````bash
$ kubectl apply -f .\kubernetes\deployments\deployment-mysql.yml
deployment.apps/d-mysql configured
````

üìåNota t√©cnica:
> Este comando actualiz√≥ el Deployment existente `d-mysql` para incluir la nueva configuraci√≥n de volumen.
> El mensaje `configured` indica que el Deployment `d-mysql` ya exist√≠a en el cl√∫ster y fue modificado con la nueva
> configuraci√≥n.
>
> Si el Deployment no hubiera existido previamente, el mensaje habr√≠a sido `created`.
>
> Para verificar que el volumen fue aplicado correctamente, se recomienda revisar el estado del pod con:
> `kubectl describe pod <nombre-del-pod>`


Volvemos a listar los pods, notemos que algo curioso pas√≥ en el pod de mysql.

````bash
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS         AGE
d-course-service-5f7bdcf6dc-ln4pg   1/1     Running   4 (6m16s ago)    5d3h
d-mysql-7c844dccd9-8fch2            1/1     Running   0                40s
d-postgres-69f7b66859-9j8h7         1/1     Running   5 (6m16s ago)    5d23h
d-user-service-7bc4ffb6df-4zzj8     1/1     Running   17 (4m ago)      5d23h
d-user-service-7bc4ffb6df-fg95f     1/1     Running   17 (4m ago)      5d23h
d-user-service-7bc4ffb6df-pvm4w     1/1     Running   17 (4m ago)      5d23h
d-user-service-7bc4ffb6df-pvm9t     1/1     Running   17 (3m58s ago)   5d23h
d-user-service-7bc4ffb6df-r54hn     1/1     Running   17 (4m1s ago)    5d23h
````

Despu√©s de aplicar la nueva configuraci√≥n, se observa que el nombre del Pod cambi√≥:

````bash
Antes: d-mysql-7b947869d5-fshhr
Despu√©s: d-mysql-7c844dccd9-8fch2
````

üìå Explicaci√≥n T√©cnica:

- Cuando se modifica la plantilla del Deployment (como al agregar vol√∫menes), Kubernetes crea un nuevo `ReplicaSet` con
  un hash distinto.
- Este nuevo `ReplicaSet` genera Pods actualizados con la nueva configuraci√≥n.
- Los Pods anteriores son eliminados autom√°ticamente para mantener la consistencia.
- Este proceso garantiza que los cambios aplicados al Deployment se reflejen correctamente en ejecuci√≥n.

Si en este punto hacemos un `describe` del nuevo pod de mysql veremos que el volumen ya est√° configurado.

````bash
$ kubectl describe pod d-mysql-7c844dccd9-8fch2
Name:             d-mysql-7c844dccd9-8fch2
Namespace:        default
Priority:         0
Service Account:  default
Node:             minikube/192.168.49.2
Start Time:       Thu, 24 Jul 2025 23:25:50 -0500
Labels:           app=d-mysql
                  pod-template-hash=7c844dccd9
Annotations:      <none>
Status:           Running
IP:               10.244.0.55
IPs:
  IP:           10.244.0.55
Controlled By:  ReplicaSet/d-mysql-7c844dccd9
Containers:
  c-mysql:
    Container ID:   docker://1ba4a4d1dceddf2523b4a24aa77280b368cb3eac4709e5d3130f6c3fd3a74e05
    Image:          mysql:8.0.41-debian
    Image ID:       docker-pullable://mysql@sha256:b2252987e0ecdb820e96928948ac3bca1adcd2b4a2a2c7b0d7ea78f77a9dc6ac
    Port:           3306/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Thu, 24 Jul 2025 23:25:51 -0500
    Ready:          True
    Restart Count:  0
    Environment:
      MYSQL_ROOT_PASSWORD:  magadiflo
      MYSQL_DATABASE:       db_user_service
      MYSQL_USER:           admin
      MYSQL_PASSWORD:       magadiflo
    Mounts:
      /var/lib/mysql from mysql-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vfjlx (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  mysql-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/mysql
    HostPathType:  DirectoryOrCreate
  kube-api-access-vfjlx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    Optional:                false
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  16m   default-scheduler  Successfully assigned default/d-mysql-7c844dccd9-8fch2 to minikube
  Normal  Pulled     16m   kubelet            Container image "mysql:8.0.41-debian" already present on machine
  Normal  Created    16m   kubelet            Created container: c-mysql
  Normal  Started    16m   kubelet            Started container c-mysql
````

## Creando tablas en el contenedor del microservicio de usuarios

Hasta este punto tenemos creado el pod de mysql junto al volumen del tipo hostPath, pero aun nuestra base de datos est√°
vac√≠a, no tiene ninguna tabla. En ese sentido, si realizamos una petici√≥n al servicio de usuarios, simplemente
recibiremos un error, ya que no hay tablas en nuestra base de datos.

Para crear nuestras tablas, debemos levantar un nuevo pod de nuestro microservicio de usuarios, pero previamente hay
que eliminar las 5 instancias que tenemos en ejecuci√≥n de este microservicio y aprovecharemos a definir una √∫nica
instancia.

Entonces, procedemos a eliminar todos los deployments de `d-user-service`.

````bash
$ kubectl delete -f .\kubernetes\deployments\deployment-user.yml
deployment.apps "d-user-service" deleted
````

Si listamos los deployments, vemos que ya no tenemos el de usuarios.

````bash
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
d-course-service   1/1     1            1           5d4h
d-mysql            1/1     1            1           5d23h
d-postgres         1/1     1            1           5d23h
````

Modificamos el `deployment-user.yml` para tener una sola r√©plica de pods (1 solo pod).

````yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: d-user-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: d-user-service
...
````

Habiendo eliminado los pods de usuarios y actualizado la r√©plica a un solo pod, procedemos a `aplicar` el
`deployment-user.yml` para que:

- Se cree un solo pod
- Se creen las tablas en el contenedor de dicho pod

````bash
$ kubectl apply -f .\kubernetes\deployments\deployment-user.yml
deployment.apps/d-user-service created
````

Si listamos nuevamente los deployments veremos que el deployment `d-user-service` fue creado hace `35s`.

````bash
kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
d-course-service   1/1     1            1           5d4h
d-mysql            1/1     1            1           5d23h
d-postgres         1/1     1            1           5d23h
d-user-service     1/1     1            1           35s 
````

Ahora, procedemos a listar todos los pods existentes

````bash
$ kubectl get pods
NAME                                READY   STATUS    RESTARTS      AGE
d-course-service-5f7bdcf6dc-ln4pg   1/1     Running   4 (37m ago)   5d4h
d-mysql-7c844dccd9-8fch2            1/1     Running   0             32m
d-postgres-69f7b66859-9j8h7         1/1     Running   5 (37m ago)   5d23h
d-user-service-7bc4ffb6df-sdcpp     1/1     Running   0             4m35s
````

Si hacemos un `logs` al pod del deployment `d-user-service`, comprobaremos que las tablas se han creado correctamente.

````bash
$ kubectl logs d-user-service-7bc4ffb6df-sdcpp

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/

 :: Spring Boot ::                (v3.4.5)

2025-07-25T04:53:42.317Z  INFO 1 --- [user-service] [           main] d.m.user.app.UserServiceApplication      : Starting UserServiceApplication v0.0.1-SNAPSHOT using Java 21.0.7 with PID 1 (/app/BOOT-INF/classes started by root in /app) 2025-07-25T04:53:42.321Z DEBUG 1 --- [user-service] [           main] d.m.user.app.UserServiceApplication      : Running with Spring Boot v3.4.5, Spring v6.2.6
2025-07-25T04:53:42.323Z  INFO 1 --- [user-service] [           main] d.m.user.app.UserServiceApplication      : The following 1 profile is active: "default"
...
2025-07-25T04:53:49.120Z  INFO 1 --- [user-service] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port 8001 (http)
...
2025-07-25T04:53:52.424Z  INFO 1 --- [user-service] [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
2025-07-25T04:53:53.048Z  INFO 1 --- [user-service] [           main] com.zaxxer.hikari.pool.HikariPool        : HikariPool-1 - Added connection com.mysql.cj.jdbc.ConnectionImpl@8ecc457
2025-07-25T04:53:53.051Z  INFO 1 --- [user-service] [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
2025-07-25T04:53:53.166Z  INFO 1 --- [user-service] [           main] org.hibernate.orm.connections.pooling    : HHH10001005: Database info:
        Database JDBC URL [Connecting through datasource 'HikariDataSource (HikariPool-1)']
        Database driver: undefined/unknown
        Database version: 8.0.41
        Autocommit mode: undefined/unknown
        Isolation level: undefined/unknown
        Minimum pool size: undefined/unknown
        Maximum pool size: undefined/unknown
2025-07-25T04:53:53.353Z  INFO 1 --- [user-service] [           main] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration)
2025-07-25T04:53:53.390Z DEBUG 1 --- [user-service] [           main] org.hibernate.SQL                        :
    create table users (
        id bigint not null auto_increment,
        email varchar(255) not null,
        name varchar(255) not null,
        password varchar(255) not null,
        primary key (id)
    ) engine=InnoDB
2025-07-25T04:53:53.439Z DEBUG 1 --- [user-service] [           main] org.hibernate.SQL                        :
    alter table users
       drop index UK6dotkott2kjsp8vw4d0m25fb7
2025-07-25T04:53:53.660Z DEBUG 1 --- [user-service] [           main] org.hibernate.SQL                        :
    alter table users
       add constraint UK6dotkott2kjsp8vw4d0m25fb7 unique (email)
2025-07-25T04:53:53.693Z  INFO 1 --- [user-service] [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
...
2025-07-25T04:53:59.925Z  INFO 1 --- [user-service] [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port 8001 (http) with context path '/'
2025-07-25T04:53:59.945Z  INFO 1 --- [user-service] [           main] d.m.user.app.UserServiceApplication      : Started UserServiceApplication in 18.789 seconds (process running for 20.476)
````
